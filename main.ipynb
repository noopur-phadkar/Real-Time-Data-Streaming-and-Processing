{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a39961-ef88-4c98-bdaf-32599b4e4106",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Real-time Data Processing with Azure Databricks (and Event Hubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49397d51-5412-4079-9159-06307954a7c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.shell import spark\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create catalog streaming;\")\n",
    "except:\n",
    "    print('check if catalog already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.bronze;\")\n",
    "except:\n",
    "    print('check if bronze schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.silver\")\n",
    "except:\n",
    "    print('check if silver schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.gold;\")\n",
    "except:\n",
    "    print('check if gold schema already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b61da0-3b55-4941-8436-2411646fa4f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3397f5f1-841b-4911-9099-8b99b92ab7c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your Event Hub namespace, name, and key\n",
    "connectionString = \"Paste connection string here\"\n",
    "eventHubName = \"Type event hub name here\"\n",
    "\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString' : spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n",
    "  'eventhubs.eventHubName': eventHubName\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b28ff3-52b8-4a3f-ad67-758854096a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading and writing the stream to the bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab57af3-36c7-4ffa-b13f-a833d7a2dc53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading stream: Load data from Azure Event Hub into DataFrame 'df' using the previously configured settings\n",
    "df = spark.readStream.format(\"eventhubs\").options(**ehConf).load()\n",
    "# df.display()\n",
    "\n",
    "# Writing stream: Persist the streaming data to a Delta table 'streaming.bronze.weather' in 'append' mode with checkpointing\n",
    "df.writeStream.option(\"checkpointLocation\", \"/mnt/streaming/bronze/weather\").outputMode(\"append\").format(\"delta\").toTable(\"streaming.bronze.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbee364e-6b7e-420f-8b98-8a12d663d880",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b661190-f917-448b-9093-1017dde3bb25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = StructType([\n",
    "    StructField(\"temperature\", IntegerType()),\n",
    "    StructField(\"humidity\", IntegerType()),\n",
    "    StructField(\"windSpeed\", IntegerType()),\n",
    "    StructField(\"windDirection\", StringType()),\n",
    "    StructField(\"precipitation\", IntegerType()),\n",
    "    StructField(\"conditions\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18db15d2-f3d1-4dd7-b5aa-ef421208b46c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading and Transforming: Load streaming data from the 'streaming.bronze.weather' Delta table, cast 'body' to string, parse JSON, and select specific fields\n",
    "df = spark.readStream.format(\"delta\").table(\"streaming.bronze.weather\").withColumn(\"body\", col(\"body\").cast(\"string\"))\\\n",
    "    .withColumn(\"body\",from_json(col(\"body\"), json_schema)).select(\"body.temperature\", \"body.humidity\", \"body.windSpeed\", \"body.windDirection\", \"body.precipitation\", \"body.conditions\", col(\"enqueuedTime\").alias('timestamp'))\n",
    "# df.display()\n",
    "\n",
    "# Writing stream: Save the transformed data to the 'streaming.silver.weather' Delta table in 'append' mode with checkpointing for data reliability\n",
    "df.writeStream.option(\"checkpointLocation\", \"/mnt/streaming/silver/weather\").outputMode(\"append\").format(\"delta\").toTable(\"streaming.silver.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4078a144-9652-490b-adf3-348ad8780e95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4b337b-6bb7-490d-b1d3-2e8b83456661",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregating Stream: Read from 'streaming.silver.weather', apply watermarking and windowing, and calculate average weather metrics\n",
    "df = spark.readStream.format(\"delta\").table(\"streaming.silver.weather\").withWatermark(\"timestamp\", \"5 minutes\").groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .agg(avg(\"temperature\").alias('temperature'), avg(\"humidity\").alias('humidity'), avg(\"windSpeed\").alias('windSpeed'), avg(\"precipitation\").alias('precipitation')).select('window.start', 'window.end', 'temperature', 'humidity', 'windSpeed', 'precipitation')\n",
    "# df.display()\n",
    "\n",
    "# Writing Aggregated Stream: Store the aggregated data in 'streaming.gold.weather_aggregated' with checkpointing for data integrity\n",
    "df.writeStream.option(\"checkpointLocation\", \"/mnt/streaming/weather_summary\").outputMode(\"append\").format(\"delta\").toTable(\"streaming.gold.weather_summary\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4016826182764860,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Real-time Data Processing with Azure Databricks (and Event Hubs)",
   "widgets": {}
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
