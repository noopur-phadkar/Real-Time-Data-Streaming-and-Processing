{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a39961-ef88-4c98-bdaf-32599b4e4106",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Real-time Data Processing with Azure Databricks (and Event Hubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T21:25:57.681134Z",
     "start_time": "2024-03-08T21:25:57.481107Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49397d51-5412-4079-9159-06307954a7c1",
     "showTitle": false,
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2024-03-08T21:26:21.703868Z",
     "start_time": "2024-03-08T21:26:18.490729Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/08 16:26:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.6 (default, Aug 11 2023 19:44:49)\n",
      "Spark context Web UI available at http://noopurs-mbp:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1709933180665).\n",
      "SparkSession available as 'spark'.\n",
      "check if catalog already exists\n",
      "check if bronze schema already exists\n",
      "check if silver schema already exists\n",
      "check if gold schema already exists\n"
     ]
    }
   ],
   "source": [
    "from pyspark.shell import spark\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create catalog streaming;\")\n",
    "except:\n",
    "    print('check if catalog already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.bronze;\")\n",
    "except:\n",
    "    print('check if bronze schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.silver\")\n",
    "except:\n",
    "    print('check if silver schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.gold;\")\n",
    "except:\n",
    "    print('check if gold schema already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b61da0-3b55-4941-8436-2411646fa4f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3397f5f1-841b-4911-9099-8b99b92ab7c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your Event Hub namespace, name, and key\n",
    "connectionString = \"Paste connection string here\"\n",
    "eventHubName = \"Type event hub name here\"\n",
    "\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n",
    "  'eventhubs.eventHubName': eventHubName\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b28ff3-52b8-4a3f-ad67-758854096a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading and writing the stream to the bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab57af3-36c7-4ffa-b13f-a833d7a2dc53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading stream: Load data from Azure Event Hub into DataFrame 'df' using the previously configured settings\n",
    "df = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load() \\\n",
    "\n",
    "# Displaying stream: Show the incoming streaming data for visualization and debugging purposes\n",
    "df.display()\n",
    "\n",
    "# Writing stream: Persist the streaming data to a Delta table 'streaming.bronze.weather' in 'append' mode with checkpointing\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/bronze/weather\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.bronze.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbee364e-6b7e-420f-8b98-8a12d663d880",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c824846-2235-4e3b-8e75-68281293d50b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Defining the schema for the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b661190-f917-448b-9093-1017dde3bb25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining the schema for the JSON object\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"temperature\", IntegerType()),\n",
    "    StructField(\"humidity\", IntegerType()),\n",
    "    StructField(\"windSpeed\", IntegerType()),\n",
    "    StructField(\"windDirection\", StringType()),\n",
    "    StructField(\"precipitation\", IntegerType()),\n",
    "    StructField(\"conditions\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcd659c9-65fc-4112-be6e-06f63196311f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading, transforming and writing the stream from the bronze to the silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18db15d2-f3d1-4dd7-b5aa-ef421208b46c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading and Transforming: Load streaming data from the 'streaming.bronze.weather' Delta table, cast 'body' to string, parse JSON, and select specific fields\n",
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .table(\"streaming.bronze.weather\")\\\n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\"))\\\n",
    "    .withColumn(\"body\",from_json(col(\"body\"), json_schema))\\\n",
    "    .select(\"body.temperature\", \"body.humidity\", \"body.windSpeed\", \"body.windDirection\", \"body.precipitation\", \"body.conditions\", col(\"enqueuedTime\").alias('timestamp'))\n",
    "\n",
    "# Displaying stream: Visualize the transformed data in the DataFrame for verification and analysis\n",
    "df.display()\n",
    "\n",
    "# Writing stream: Save the transformed data to the 'streaming.silver.weather' Delta table in 'append' mode with checkpointing for data reliability\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/silver/weather\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.silver.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4078a144-9652-490b-adf3-348ad8780e95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Gold Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c07779-5a57-4f70-8c90-34554f8a82b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading, aggregating and writing the stream from the silver to the gold layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4b337b-6bb7-490d-b1d3-2e8b83456661",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregating Stream: Read from 'streaming.silver.weather', apply watermarking and windowing, and calculate average weather metrics\n",
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .table(\"streaming.silver.weather\")\\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .agg(avg(\"temperature\").alias('temperature'), avg(\"humidity\").alias('humidity'), avg(\"windSpeed\").alias('windSpeed'), avg(\"precipitation\").alias('precipitation'))\\\n",
    "\t.select('window.start', 'window.end', 'temperature', 'humidity', 'windSpeed', 'precipitation')\n",
    "\n",
    "# Displaying Aggregated Stream: Visualize aggregated data for insights into weather trends\n",
    "df.display()\n",
    "\n",
    "# Writing Aggregated Stream: Store the aggregated data in 'streaming.gold.weather_aggregated' with checkpointing for data integrity\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/weather_summary\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.gold.weather_summary\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4016826182764860,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Real-time Data Processing with Azure Databricks (and Event Hubs)",
   "widgets": {}
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
